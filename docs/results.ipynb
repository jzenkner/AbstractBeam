{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from statannotations.Annotator import Annotator\n",
    "from pingouin import ttest, homoscedasticity, normality\n",
    "\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_plot(path, name, fig):\n",
    "    with open(path + \"tikz/\" + name + \".pkl\", 'wb') as file:\n",
    "        pickle.dump(fig, file)\n",
    "    fig.savefig(path + \"svg/\" + name + \".svg\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parent_path = \"../neurips/\"\n",
    "plot_path = \"../neurips/plots/\"\n",
    "run = '2'\n",
    "approach = \"abstractbeam\"\n",
    "testset = \"handwritten\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(parent_path + f\"{approach}/ogdomain/models{run}/model-latest.ckpt\", \"rb\") as f:\n",
    "    ckpt = torch.load(f)  # , map_location=torch.device(\"mps\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventions = ckpt[\"inventions\"]\n",
    "for inv in inventions:\n",
    "    print(inv.name, inv.program, inv.arity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constants = ckpt[\"domain\"].constants\n",
    "print(f\"initial constants: {[-1, 0, 1, 2, 3, 4]}\")\n",
    "print(f\"now: {constants}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open(parent_path + f\"{approach}/ogdomain/results{run}/run_1.json\", \"rb\") as json_file:\n",
    "    results = json.load(json_file)\n",
    "solutions = [ele for ele in results[\"results\"] if ele[\"success\"]]\n",
    "len(solutions) / len(results[\"results\"]) * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "p = '../neurips/lambdabeam/ogdomain/models2/logs/'\n",
    "for file in os.listdir(p): \n",
    "    # Initialize an event accumulator\n",
    "    event_AB = EventAccumulator(p + file)\n",
    "    event_AB.Reload()\n",
    "    try:\n",
    "        scalars = event_AB.Scalars('eval/succ')\n",
    "        print(file)\n",
    "    except KeyError:\n",
    "        tst = 1\n",
    "        # print(\"empty\")\n",
    "        # print(file)\n",
    "        # os.remove(lb_file + file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"path_dict = {\"abstractbeam fold 1\": [\"events.out.tfevents.1714053475.dws-07.3890530.0\", \"events.out.tfevents.1714032509.dws-07.3771435.0\"],\n",
    "             \"abstractbeam fold 2\": [\"events.out.tfevents.1714457794.dws-08.391667.0\", \"events.out.tfevents.1715340297.dws-student-01.3676656.0\", \"events.out.tfevents.1715500982.dws-student-01.220501.0\", \"events.out.tfevents.1715599491.dws-student-01.598324.0\", \"events.out.tfevents.1715754350.dws-student-01.1001414.0\"],\n",
    "             \"abstractbeam fold 1 og\": [\"events.out.tfevents.1714975955.dws-08.865485.0\", \"events.out.tfevents.1715072169.dws-08.983221.0\"],\n",
    "             \"lambdabeam fold 1\": [\"events.out.tfevents.1713428257.dws-06.1874500.0\", \"events.out.tfevents.1713532162.dws-06.528186.0\"],\n",
    "             \"lambdabeam fold 2\": [\"events.out.tfevents.1714628885.dws-06.876364.0\", \"events.out.tfevents.1715683029.dws-student-01.754111.0\"],\n",
    "             \"lambdabeam fold 1 og\": []\n",
    "             }\"\"\"\n",
    "path_dict= {\n",
    "    \"ab fold 1\": [\"events.out.tfevents.1715801783.dws-student-01.1781218.0\", \"events.out.tfevents.1715839068.dws-student-01.1849346.0\"],\n",
    "    \"ab fold 2\": [\"events.out.tfevents.1715801919.dws-student-01.1785344.0\", \"events.out.tfevents.1715885186.dws-student-01.2063158.0\"],\n",
    "    \"lb fold 1\": [\"events.out.tfevents.1715858660.dws-08.2976407.0\"],\n",
    "    \"lb fold 2\": [\"events.out.tfevents.1715853059.dws-student-01.1937098.0\"]\n",
    "}\n",
    "fold = 2\n",
    "# Path to your event file or directory\n",
    "ab_file = '../neurips/abstractbeam/ogdomain/models/logs/' if fold == 1 else '../neurips/abstractbeam/ogdomain/models2/logs/'\n",
    "scalars_AB, times = [], []\n",
    "for file in path_dict[f\"ab fold {fold}\"]: \n",
    "    # Initialize an event accumulator\n",
    "    event_AB = EventAccumulator(ab_file + file)\n",
    "    event_AB.Reload()\n",
    "    scalars = event_AB.Scalars('eval/succ')\n",
    "    times += [ele.wall_time for ele in scalars]\n",
    "    scalars_AB += [ele.value for ele in scalars]\n",
    "print(np.diff(np.array(times)) / 3600)\n",
    "inds = np.argsort(times)\n",
    "scalars_AB = np.array(scalars_AB)[inds]\n",
    "\n",
    "lb_file = '../neurips/lambdabeam/ogdomain/models/logs/' if fold == 1 else '../neurips/lambdabeam/ogdomain/models2/logs/'\n",
    "scalars_LB, times = [], []\n",
    "for file in path_dict[f\"lb fold {fold}\"]: \n",
    "    # Initialize an event accumulator\n",
    "    event_LB = EventAccumulator(lb_file + file)\n",
    "    event_LB.Reload()\n",
    "    try:\n",
    "        scalars = event_LB.Scalars('eval/succ')\n",
    "    except KeyError:\n",
    "        print(file)\n",
    "    times += [ele.wall_time for ele in scalars]\n",
    "    scalars_LB += [ele.value for ele in scalars]\n",
    "print(np.diff(np.array(times)) / 3600)\n",
    "max_len = max(len(scalars_AB), len(scalars_LB))\n",
    "fig, ax = plt.subplots()\n",
    "df = pd.DataFrame.from_dict({\"AbstractBeam\": np.array(list(scalars_AB) + [np.nan] * (max_len - len(scalars_AB))) * 100, \"LambdaBeam\": np.array(list(scalars_LB) + [np.nan] * (max_len - len(scalars_LB))) * 100})\n",
    "sns.lineplot(data=df, x=np.arange(len(df)), y=df[\"AbstractBeam\"], label=\"AbstractBeam\")\n",
    "ax.text(len(scalars_AB) - 0.75, max(df[\"AbstractBeam\"])+0.1, f'{max(df[\"AbstractBeam\"]):.2f}%', ha='right', va='bottom', fontsize=8)\n",
    "sns.lineplot(data=df, x=np.arange(len(df)), y=df[\"LambdaBeam\"], label=\"LambdaBeam\")\n",
    "ax.text(len(scalars_LB) - 0.75, max(df[\"LambdaBeam\"])+0.1, f'{max(df[\"LambdaBeam\"]):.2f}%', ha='right', va='bottom', fontsize=8)\n",
    "ax.set_ylabel(\"Success rate [%]\")\n",
    "ax.set_xlabel(\"Iteration\")\n",
    "# fig.legend()\n",
    "fig.tight_layout()\n",
    "# save_plot(plot_path, \"performance_iteration\", plt.gcf())\n",
    "print(max(df[\"AbstractBeam\"]), max(df[\"LambdaBeam\"]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = f\"../neurips/abstractbeam/eval/fold2/{testset}/\"\n",
    "succ_ab = []\n",
    "for e, file_path in enumerate(os.listdir(path)):\n",
    "    with open(path + file_path) as f:\n",
    "        results = json.load(f)\n",
    "        l = len(results[\"results\"])\n",
    "        results = results[\"num_tasks_solved\"]\n",
    "    succ_ab.append(results / l * 100)\n",
    "\n",
    "sns.set_palette(\"deep\")\n",
    "path = f\"../neurips/lambdabeam/eval/{testset}/\"\n",
    "succ_lb = []\n",
    "for e, file_path in enumerate(os.listdir(path)):\n",
    "    with open(path + file_path) as f:\n",
    "        results = json.load(f)[\"num_tasks_solved\"]\n",
    "    succ_lb.append(results / l * 100)\n",
    "print(np.mean(succ_ab), np.std(succ_ab), np.mean(succ_lb), np.std(succ_lb))\n",
    "df = pd.DataFrame.from_dict(\n",
    "    {\"Success rate [%]\": succ_ab + succ_lb, \"Approach\": [\"AbstractBeam\"] * len(succ_ab) + [\"LambdaBeam\"] * len(succ_lb)})\n",
    "ax = sns.barplot(data=df, x=\"Approach\", y=\"Success rate [%]\")\n",
    "ax.patches[1].set_color(sns.color_palette(desat=0.75)[1])\n",
    "\n",
    "print(normality(df, dv=\"Success rate [%]\", group=\"Approach\"))\n",
    "print(homoscedasticity(df, dv=\"Success rate [%]\", group=\"Approach\"))\n",
    "\n",
    "annotator = Annotator(ax, [(\"AbstractBeam\", \"LambdaBeam\")], data=df, x=\"Approach\", y=\"Success rate [%]\")\n",
    "annotator.configure(test='t-test_ind', text_format='star', loc='outside')\n",
    "a = annotator.apply_and_annotate()\n",
    "# save_plot(plot_path, f\"performance_{testset}\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from crossbeam.data.deepcoder.solution_weight import solution_weight\n",
    "def load_and_process_results(file_paths, path, key=\"elapsed_time\"):\n",
    "    l = []\n",
    "    for e, file_path in enumerate(file_paths):\n",
    "        with open(path + file_path) as f:\n",
    "            results = json.load(f)[\"results\"]\n",
    "        if key == \"total_num_values_explored\":\n",
    "            num_programs, success = zip(*[(sol[\"stats\"][key], sol[\"success\"]) for sol in results if sol[\"stats\"][\"total_num_values_explored\"] and sol[\"elapsed_time\"] <= 100])\n",
    "        elif key == \"elapsed_time\":\n",
    "            num_programs, success = zip(*[(sol[\"elapsed_time\"], sol[\"success\"]) for sol in results if sol[\"elapsed_time\"] and sol[\"elapsed_time\"] <= 100])\n",
    "        elif key == \"Solution weight\":\n",
    "            num_programs, success = zip(*[(solution_weight(sol[\"solution\"], domain=ckpt[\"domain\"]), sol[\"success\"]) for sol in results if sol[\"elapsed_time\"] and sol[\"elapsed_time\"] <= 100])\n",
    "            \n",
    "        sorted_data = sorted(zip(num_programs, success))\n",
    "        sorted_time_taken, sorted_tasks_solved = zip(*sorted_data)\n",
    "        cumulative_tasks = [sum(sorted_tasks_solved[:i+1]) / len(results) * 100 for i in range(len(sorted_tasks_solved))]\n",
    "        if key == \"elapsed_time\":\n",
    "            sorted_time_taken = list(sorted_time_taken) + [100]\n",
    "            cumulative_tasks.append(cumulative_tasks[-1])\n",
    "            x_key = \"Elapsed time\"\n",
    "        elif key == \"total_num_values_explored\":\n",
    "            sorted_time_taken = list(sorted_time_taken)\n",
    "            x_key = \"Number of explored values\"\n",
    "        elif key == \"Solution weight\":\n",
    "            sorted_time_taken = list(sorted_time_taken)\n",
    "            x_key = key\n",
    "        \n",
    "        l.append(pd.DataFrame.from_dict({x_key: sorted_time_taken, \"Success rate [%]\": cumulative_tasks}))\n",
    "    if key == \"Solution weight\":\n",
    "        return pd.concat(l).sort_values(x_key)\n",
    "    dfs = pd.concat(l).sort_values(x_key)\n",
    "    dfs[x_key] = dfs[x_key].rolling(window=3).mean()\n",
    "    dfs = dfs.drop_duplicates(subset=[x_key])\n",
    "    dfs = dfs.set_index(x_key)\n",
    "    \n",
    "    dfs['Moving Max'] = dfs['Success rate [%]'].rolling(window=3).max()\n",
    "    dfs['Moving Min'] = dfs['Success rate [%]'].rolling(window=3).min()\n",
    "    dfs['Success rate [%]'] = dfs['Success rate [%]'].rolling(window=3).mean()\n",
    "    return dfs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "df_ab = load_and_process_results(os.listdir(f\"../neurips/abstractbeam/eval/fold2/{testset}/\"), f\"../neurips/abstractbeam/eval/fold2/{testset}/\")\n",
    "df_lb = load_and_process_results(os.listdir(f\"../neurips/lambdabeam/eval/{testset}/\"), f\"../neurips/lambdabeam/eval/{testset}/\")\n",
    "\n",
    "sns.lineplot(data=df_ab, x=\"Elapsed time\", y=\"Success rate [%]\", label=\"AbstractBeam\", ax=ax)\n",
    "plt.fill_between(df_ab.index, df_ab[\"Moving Min\"], df_ab[\"Moving Max\"], alpha=0.3)\n",
    "\n",
    "sns.lineplot(data=df_lb, x=\"Elapsed time\", y=\"Success rate [%]\", label=\"LambdaBeam\", ax=ax)\n",
    "ax.set_xlabel(\"Elapsed time [s]\")\n",
    "plt.fill_between(df_lb.index, df_lb[\"Moving Min\"], df_lb[\"Moving Max\"], alpha=0.3)\n",
    "#save_plot(plot_path, f\"performance_time_{testset}\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "df_ab = load_and_process_results(os.listdir(f\"../neurips/abstractbeam/eval/fold2/{testset}/\"), f\"../neurips/abstractbeam/eval/fold2/{testset}/\", key=\"total_num_values_explored\")\n",
    "df_lb = load_and_process_results(os.listdir(f\"../neurips/lambdabeam/eval//{testset}/\"), f\"../neurips/lambdabeam/eval/{testset}/\", key=\"total_num_values_explored\")\n",
    "\n",
    "sns.lineplot(data=df_ab, x=\"Number of explored values\", y=\"Success rate [%]\", label=\"AbstractBeam\", ax=ax)\n",
    "plt.fill_between(df_ab.index, df_ab[\"Moving Min\"], df_ab[\"Moving Max\"], alpha=0.3)\n",
    "sns.lineplot(data=df_lb, x=\"Number of explored values\", y=\"Success rate [%]\", label=\"LambdaBeam\", ax=ax)\n",
    "plt.fill_between(df_lb.index, df_lb[\"Moving Min\"], df_lb[\"Moving Max\"], alpha=0.3)\n",
    "#save_plot(plot_path, f\"performance_programs_{testset}\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_data_over_length(file_paths, path, key=\"Success rate [%]\"):\n",
    "    \n",
    "    df = {\"Solution weight\": [], key: []}\n",
    "    for e, file_path in enumerate(file_paths):\n",
    "        with open(path + file_path) as f:\n",
    "            results = json.load(f)[\"results\"]\n",
    "\n",
    "        if key == \"Success rate [%]\":\n",
    "            success = np.array([sol[\"success\"] if sol[\"success\"] else False for sol in results])\n",
    "            x_axis = [solution_weight(sol[\"task_solution\"], domain=ckpt[\"domain\"]) for sol in results]\n",
    "            # print([sol[\"task_solution\"] for sol in results if solution_weight(sol[\"task_solution\"], domain=ckpt[\"domain\"]) == 8])\n",
    "            df[\"Solution weight\"] += x_axis\n",
    "            df[\"Success rate [%]\"] += list(success)\n",
    "        elif key == \"Abstraction usage [%]\":\n",
    "            sols = [sol for sol in results if sol[\"solution\"]] \n",
    "            abs_used = np.array([True if \"fn_\" in str(sol[\"solution\"]) or \"0\" in str(sol[\"solution\"]) else False for sol in sols])\n",
    "            x_axis = [solution_weight(sol[\"task_solution\"], domain=ckpt[\"domain\"]) for sol in sols]\n",
    "            # print([(s[\"solution\"], weight, s[\"solution_weight\"], np.mean(abs_used)) for s, weight in zip(sols, x_axis) if weight == 8])\n",
    "            df[\"Solution weight\"] += x_axis\n",
    "            df[\"Abstraction usage [%]\"] += list(abs_used)\n",
    "        elif key == \"Elapsed time [s]\":\n",
    "            success = np.array([sol[\"elapsed_time\"] for sol in results]) \n",
    "            x_axis = [solution_weight(sol[\"task_solution\"], domain=ckpt[\"domain\"]) for sol in results]\n",
    "            df[\"Solution weight\"] += x_axis\n",
    "            df[\"Elapsed time [s]\"] += list(success)\n",
    "        elif key == \"Average number of used abstractions\":\n",
    "            success = np.array([str(sol[\"solution\"]).count(\"fn_\") + str(sol[\"solution\"]).count(\"0\") for sol in results if sol[\"solution\"]])\n",
    "            x_axis = [solution_weight(sol[\"task_solution\"], domain=ckpt[\"domain\"]) for sol in results if sol[\"solution\"]]\n",
    "            df[\"Solution weight\"] += x_axis\n",
    "            df[\"Average number of used abstractions\"] += list(success)\n",
    "\n",
    "    dfs = pd.DataFrame.from_dict(df)\n",
    "    return dfs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key = \"Abstraction usage [%]\"\n",
    "testset = \"handwritten\"\n",
    "bined_df_ab = process_data_over_length(os.listdir(f\"../neurips/abstractbeam/eval/fold2/{testset}/\"), f\"../neurips/abstractbeam/eval/fold2/{testset}/\", key=key)\n",
    "bined_df_lb = process_data_over_length(os.listdir(f\"../neurips/lambdabeam/eval/{testset}/\"), f\"../neurips/lambdabeam/eval/{testset}/\", key=key)\n",
    "df_merged = pd.concat([bined_df_ab, bined_df_lb], keys=['AbstractBeam', 'LambdaBeam']).reset_index()\n",
    "df_merged.drop(columns=\"level_1\", inplace=True)\n",
    "df_merged.rename(columns={'level_0': 'Approach'}, inplace=True)\n",
    "df_merged[key] *= 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "ax = sns.histplot(data=df_merged[df_merged[\"Abstraction usage [%]\"] == 1], x=\"Solution weight\", ax=ax, color=sns.color_palette()[0], discrete=True)\n",
    "\n",
    "n_prog_per_len = np.array([len(df_merged[df_merged[\"Solution weight\"] == l]) for l in range(1, 20)]) \n",
    "df_sol = df_merged[df_merged[\"Abstraction usage [%]\"] == 1]\n",
    "n_sol_per_len = np.array([len(df_sol[df_sol[\"Solution weight\"] == l]) for l in range(1, 20)])\n",
    "percentage = n_sol_per_len / n_prog_per_len * 100\n",
    "\n",
    "ax.set_ylim(0, 20, 2)\n",
    "plt.yticks(range(0, 20, 2))\n",
    "for i, p in enumerate(percentage):\n",
    "    if p != 0 and not np.isnan(p):\n",
    "        ax.text(i + 1, 1, f\"{round(p)}%\", color='black', ha=\"center\")\n",
    "\n",
    "#save_plot(plot_path, f\"performance_abstractionusage_{testset}\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "key = \"Success rate [%]\"\n",
    "bined_df_ab = process_data_over_length(os.listdir(f\"../neurips/abstractbeam/eval/{testset}/\"), f\"../neurips/abstractbeam/eval/{testset}/\", key=key)\n",
    "bined_df_lb = process_data_over_length(os.listdir(f\"../neurips/lambdabeam/eval/{testset}/\"), f\"../neurips/lambdabeam/eval/{testset}/\", key=key)\n",
    "df_merged = pd.concat([bined_df_ab, bined_df_lb], keys=['AbstractBeam', 'LambdaBeam']).reset_index()\n",
    "df_merged.drop(columns=\"level_1\", inplace=True)\n",
    "df_merged.rename(columns={'level_0': 'Approach'}, inplace=True)\n",
    "\n",
    "df_merged[key] *= 100\n",
    "ax = plt.subplot()\n",
    "ax.set_xlim(-0.5, 11.5)\n",
    "sns.barplot(data=df_merged[df_merged[\"Solution weight\"] < 12], x=\"Solution weight\", y=key, hue=\"Approach\", ax=ax)\n",
    "\n",
    "pairs = [((i, \"AbstractBeam\"), (i, \"LambdaBeam\")) for i in range(4, 12)]\n",
    "annotator.new_plot(ax, pairs, plot='barplot',\n",
    "               data=df_merged, x=\"Solution weight\", y=key, hue=\"Approach\", hue_order=[\"AbstractBeam\", \"LambdaBeam\"])\n",
    "a = annotator.apply_test().annotate()\n",
    "# save_plot(plot_path, f\"performance_programlength_{testset}\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = {f\"{op}\": 0 for op in ckpt[\"domain\"].operations}\n",
    "for e, file_path in enumerate(os.listdir(f\"../neurips/abstractbeam/eval/fold2/{testset}/\")):\n",
    "    with open(f\"../neurips/abstractbeam/eval/fold2/{testset}/\" + file_path) as f:\n",
    "        results = json.load(f)[\"results\"]\n",
    "        for sol in results:\n",
    "            if sol[\"success\"]:\n",
    "                for op in ckpt[\"domain\"].operations:\n",
    "                    if str(op) in str(sol[\"solution\"]):\n",
    "                        d[f\"{op}\"] += 1\n",
    "d = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "d = d.rename(columns={0: \"AbstractBeam\"})\n",
    "d2 = {f\"{op}\": 0 for op in ckpt[\"domain\"].operations}\n",
    "for e, file_path in enumerate(os.listdir(f\"../neurips/lambdabeam/eval/{testset}/\")):\n",
    "    with open(f\"../neurips/lambdabeam/eval/{testset}/\" + file_path) as f:\n",
    "        results = json.load(f)[\"results\"]\n",
    "        for sol in results:\n",
    "            if sol[\"success\"]:\n",
    "                for op in ckpt[\"domain\"].operations:\n",
    "                    if str(op) in str(sol[\"solution\"]):\n",
    "                        d2[f\"{op}\"] += 1\n",
    "d2 = pd.DataFrame.from_dict(d2, orient=\"index\")\n",
    "d2 = d2.rename(columns={0: \"LambdaBeam\"})\n",
    "d = pd.concat([d, d2], axis=1)\n",
    "d = pd.DataFrame.from_dict({\"Approach\": [\"AbstractBeam\"] * len(d) + [\"LambdaBeam\"] * len(d2), \"Count\": d[\"AbstractBeam\"].tolist() + d2[\"LambdaBeam\"].tolist(), \"Operations\": list(d.index) + list(d2.index)})\n",
    "d[\"Count\"] /= 5\n",
    "ax = plt.subplot()\n",
    "sns.barplot(data=d, x=\"Operations\", y=\"Count\", ax=ax, hue=\"Approach\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ticks = plt.xticks(rotation=90)\n",
    "#save_plot(plot_path, f\"operation_histogram_differences\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d = {f\"{op}\": 0 for op in ckpt[\"domain\"].operations if \"fn_\" not in str(op)}\n",
    "for e, file_path in enumerate(os.listdir(f\"../neurips/abstractbeam/eval/fold2/{testset}/\")):\n",
    "    with open(f\"../neurips/abstractbeam/eval/fold2/{testset}/\" + file_path) as f:\n",
    "        results = json.load(f)[\"results\"]\n",
    "        for sol in results:\n",
    "            for op in ckpt[\"domain\"].operations:\n",
    "                    if str(op) in str(sol[\"task_solution\"]) and \"fn_\" not in str(op):\n",
    "                        d[f\"{op}\"] += 1\n",
    "            \"\"\"if sol[\"success\"]:\n",
    "                for op in ckpt[\"domain\"].operations:\n",
    "                    if str(op) in str(sol[\"solution\"]):\n",
    "                        d[f\"{op}\"] += 1\"\"\"\n",
    "d = pd.DataFrame.from_dict(d, orient=\"index\")\n",
    "d = d.rename(columns={0: \"handwritten\"})\n",
    "d2 = {f\"{op}\": 0 for op in ckpt[\"domain\"].operations if \"fn_\" not in str(op)}\n",
    "for e, file_path in enumerate(os.listdir(f\"../neurips/lambdabeam/eval/synthetic/\")):\n",
    "    with open(f\"../neurips/lambdabeam/eval/synthetic/\" + file_path) as f:\n",
    "        results = json.load(f)[\"results\"]\n",
    "        for sol in results:\n",
    "            \n",
    "            for op in ckpt[\"domain\"].operations:\n",
    "                if str(op) in str(sol[\"task_solution\"]) and \"fn_\" not in str(op):\n",
    "                    d2[f\"{op}\"] += 1\n",
    "            \"\"\"\n",
    "            if sol[\"success\"]:\n",
    "                for op in ckpt[\"domain\"].operations:\n",
    "                if str(op) in str(sol[\"solution\"]):\n",
    "                    d2[f\"{op}\"] += 1\"\"\"\n",
    "d2 = pd.DataFrame.from_dict(d2, orient=\"index\")\n",
    "d2 = d2.rename(columns={0: \"synthetic\"})\n",
    "d = pd.concat([d, d2], axis=1)\n",
    "d = pd.DataFrame.from_dict({\"Dataset\": [\"handwritten\"] * len(d) + [\"synthetic\"] * len(d2), \"Count\": d[\"handwritten\"].tolist() + d2[\"synthetic\"].tolist(), \"Operations\": list(d.index) + list(d2.index)})\n",
    "d[\"Count\"] /= 5\n",
    "ax = plt.subplot()\n",
    "sns.set()\n",
    "sns.barplot(data=d, x=\"Operations\", y=\"Count\", ax=ax, hue=\"Dataset\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ticks = plt.xticks(rotation=90)\n",
    "#save_plot(plot_path, f\"operation_histogram_differences\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bined_df_ab = process_data_over_length(os.listdir(\"../neurips/abstractbeam/eval/handwritten/\"), \"../neurips/abstractbeam/eval/handwritten/\", key=\"Abstraction usage [%]\")\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.barplot(bined_df_ab, x=\"Solution weight\", y=\"Abstraction usage [%]\", ax=ax)\n",
    "# save_plot(plot_path, \"abstraction_usage\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bined_df_ab = process_data_over_length(os.listdir(\"../neurips/abstractbeam/eval/handwritten/\"), \"../neurips/abstractbeam/eval/handwritten/\", \"Average number of used abstractions\")\n",
    "\n",
    "ax = plt.subplot()\n",
    "sns.barplot(bined_df_ab, x=\"Solution weight\", y=\"Average number of used abstractions\", ax=ax)\n",
    "# save_plot(plot_path + \"abstractionusage_programlength\", plt.gcf())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "[(inv.name, inv.program) for inv in ckpt[\"inventions\"]] + [('0', '0')]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sols = [sol[\"solution\"] for sol in solutions if sol[\"solution\"] and \"fn_\" in str(sol[\"solution\"]) or \"0\" in str(sol[\"solution\"])]\n",
    "invs = [inv.name for inv in ckpt[\"inventions\"]] + ['0']\n",
    "count = {inv: 0 for inv in invs}\n",
    "for inv in invs:\n",
    "    for sol in sols:\n",
    "        if str(inv) in str(sol):\n",
    "            count[inv] += 1\n",
    "count  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = \"../neurips/abstractbeam/results/run_1.json\"\n",
    "with open(path) as f:\n",
    "    results_ab = json.load(f)[\"results\"]\n",
    "\n",
    "path = \"../neurips/lambdabeam/results/run_1.json\"\n",
    "with open(path) as f:\n",
    "    results_lb = json.load(f)[\"results\"]\n",
    "\n",
    "for i in range(len(results_ab)):\n",
    "    if results_ab[i][\"success\"] and not results_lb[i][\"success\"]:\n",
    "        print(f\"AbstractBeam solved: {results_ab[i]['solution']}, {results_ab[i]['task_solution']}\")\n",
    "        matches = re.findall('fn_\\d+|0', results_ab[i]['solution'])\n",
    "        if len(matches) > 0:\n",
    "            print(rf\"used abstraction(s): {matches}\")\n",
    "    if not results_ab[i][\"success\"] and results_lb[i][\"success\"]:\n",
    "        print(f\"LambdaBeam solved: {results_lb[i]['solution']}, {results_lb[i]['task_solution']}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lambdaboom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
